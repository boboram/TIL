# Ch09. 웹 크롤러 설계 

## 웹 크롤러
- 로봇 또는 스파이더라고도 부른다. 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.
- 종류
  - 검색 엔진 인덱싱 : 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.
  - 웹 아카이빙 : 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
  - 웹 마이닝 : 인터넷에서 유용한 지식 도출
  - 웹 모니터링 : 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링

## 1단계 문제 이해 및 설계 범위 확정 
- 알고리즘
  - 모든 웹 페이지 다운로드
  - URL 추출
  - 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복
- 추가 고려 사항 : 규모 확장성, 안정성, 예절, 확장성
- 개략적 규모 추정 : 얼마나 많은 웹 페이지를 수집해야 하는가? / 몇년간 저장해야 하는가? / 중복 콘텐츠 처리 여부

## 2단계 개략적 설계안 제시 및 동의 구하기 
- 시작 URL 집합 : 웹 크롤러가 크롤링을 시작하는 출발점이다.
- 미수집 URL 저장소 : 다운로드 할 URL을 저장 관리하는 컴포넌트 (Queue)
- HTML 다운로더 : 웹 페이지를 다운로드하는 컴포넌트
- 도메인 이름 변환기 : URL -> IP 변환
- 콘텐츠 파서 : 파싱, 검증
- 중복 콘텐츠인가? : 29%가량이 중복 콘텐츠, 웹 페이지 해시 값 비교
- 콘텐츠 저장소 : HTML 문서를 보관하는 시스템
- URL 추출기 : HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.
- URL 필터 : 문제가 있는 URL 제거
- 이미 방문한 URL? : 블룸 필터나 해시 테이블 사용
- URL 저장소 : 이미 방문한 URL을 보관하는 저장소

## 상세 설계
- DFS vs BFS
  - DFS는 그래프 크기가 클 경우 어느 정도로 깊숙이 가게 될지 가늠하기 어려워서 좋은 방법은 아니다. 
  - 웹 크롤러는 보통 BFS를 사용한다. (QUEUE)
    - 한 페이지에서 나오는 링크의 상당수가 같은 서버로 되돌아가서 예의 없는 크롤러로 간주된다.
- 미수집 URL 저장소 : 예의 문제를 해결 가능 
- 미수집 URL 저장소를 위한 지속성 저장장치 : 메모리 버퍼에 큐를 둔다.
- HTML 다운로더 : HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.
  - Robots.txt : 크롤러가 수집해도 되는 페이지 목록
  - 성능 최적화 : 분산 크롤링, 도메인 이름 변환 결과 캐시, 지역성, 짧은 타임 아웃
  - 안정성 : 안정해시
  - 확장성
- 문제 있는 콘텐츠 감지 및 회피
  - 중복 콘텐츠 : 30% 가량은 중복콘텐츠, 해시나 체크섬을 사용하면 중복 콘텐츠를 보다 쉽게 탐지 가능
  - 거미 덫 : 무한루프에 빠지는 현상 -> URL 최대 길이 제한하기
  - 데이터 노이즈 : 광고, 스팸 URL 등은 필요 없기에 제외 
